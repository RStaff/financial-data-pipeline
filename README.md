# Financial Data Pipeline

A reference implementation of a scalable, productionâ€style data integration workflow built in Python, orchestrated with Apache Airflow, deployed via Docker, and backed by cloud infrastructure as code. This project mirrors the core responsibilities of a Data Integration Engineer: fetching raw financial data, landing it in S3, transforming and loading into a relational store, and scheduling everything in an automated DAG.
---

## ğŸ“‚ Repository Structure
.
â”œâ”€â”€ .github/
â”‚ â””â”€â”€ workflows/ # CI/CD pipelines (lint, tests, Docker build)
â”œâ”€â”€ infra/ # Terraform code for provisioning S3 bucket, IAM roles, etc.
â”œâ”€â”€ app/ # Dockerfile, entrypoint scripts, containerized helpers
â”œâ”€â”€ src/ # Python modules
â”‚ â”œâ”€â”€ fetch_stock_data.py # ETL: pull daily CSV from Alpha Vantage
â”‚ â”œâ”€â”€ s3_upload.py # Upload local CSV to S3
â”‚ â””â”€â”€ db_load.py # Read S3 CSV and append into PostgreSQL
â”œâ”€â”€ airflow_dags/ # Airflow DAG definitions
â”‚ â””â”€â”€ stock_pipeline_dag.py
â”œâ”€â”€ Dockerfile # Containerizes the Python & Airflow environment
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # This documentation

---

## ğŸš€ Features

- **End-to-End ETL**  
  Fetch equity price data, push to AWS S3, and load into Postgres.
- **Orchestration**  
  Define and schedule a daily DAG in Apache Airflow.
- **Infrastructure as Code**  
  Terraform configs to provision S3 buckets and IAM roles.
- **Containerized Development**  
  Dockerfile for consistent local/dev/test environments.
- **CI/CD Integration**  
  GitHub Actions pipelines for linting, testing, and pushing Docker images.

---

## ğŸ› ï¸ Getting Started

### Prerequisites

- **Python 3.8+**  
- **Docker & Docker Compose**  
- **Terraform 0.13+**  
- **Airflow 2.x** (or the Docker container will spin it up)  
- AWS credentials with permissions to create/read/write S3, and an accessible Postgres instance.

### Quick Setup

1. **Clone & enter**  
   ```bash
   git clone https://github.com/RStaff/financial-data-pipeline.git
   cd financial-data-pipeline

   cd infra
terraform init
terraform apply   # provisions S3 bucket + IAM roles

cd ..
docker-compose up -d --build
ALPHAVANTAGE_API_KEY=YOUR_KEY
AWS_ACCESS_KEY_ID=â€¦
AWS_SECRET_ACCESS_KEY=â€¦
S3_BUCKET_NAME=provisioned-bucket
POSTGRES_CONN=postgresql://user:pass@db:5432/yourdb
PROJECT_PATH=/opt/airflow

**Trigger the DAG**
Open http://localhost:8080 â†’ unpause stock_data_pipeline â†’ trigger it manually or wait for the daily schedule.

ğŸ” **How It Works**
fetch_stock_data.py
Hits Alpha Vantageâ€™s REST API, saves a timestamped CSV to /tmp.

s3_upload.py
Uses boto3 to upload the latest CSV to your S3 bucket.

db_load.py
Reads that CSV from S3 and appends the records to the stock_prices table in Postgres.

stock_pipeline_dag.py
Defines a threeâ€step Airflow DAG (fetch â†’ upload â†’ load) scheduled at @daily.

# financial-data-pipeline
